{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv('dataset/stack-overflow-data.csv')\n",
    "        # Extract unique classes\n",
    "        self.tags = self.data.tags.unique()\n",
    "        \n",
    "    def plot_figures(self):\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            self.tags.value_counts().plot(kind='bar')\n",
    "            \n",
    "    def clean_text(self):\n",
    "            start = time.time()\n",
    "            # HTML decoding, Default parser is lxml\n",
    "            self.data['post'] = self.data['post'].apply(lambda text: self.filter_data(text))\n",
    "            end = time.time()\n",
    "            print(\"Time Taken: \" + str(end - start))\n",
    "            #print(self.data['post'][:20])\n",
    "            return self.data\n",
    "        \n",
    "    def filter_data(self, text):\n",
    "        # Remove parentheses, brackets and special symbols from input\n",
    "        REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,; ]')\n",
    "        # Don't remove numbers, lower case characters, #, +, _ or space\n",
    "        # (because they are found in calsses such as: \"c++, c#\")\n",
    "        BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        # Transform words to lower case letters\n",
    "        text = text.lower()\n",
    "        text = BeautifulSoup(text, 'lxml').get_text()\n",
    "        text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "        text = BAD_SYMBOLS_RE.sub('', text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # Filter stop words from text\n",
    "        text = ' '.join(lemmatizer.lemmatize(word) for word in text.split(' ') if word not in STOPWORDS)\n",
    "        # Tokenize will change classes like c#, it will split c and # in two different words\n",
    "        #text = ' '.join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if word not in STOPWORDS)\n",
    "        return text\n",
    "    \n",
    "    def word_averaging(self, wv, words):\n",
    "            all_words, mean = set(), []\n",
    "\n",
    "            for word in words:\n",
    "                if isinstance(word, np.ndarray):\n",
    "                    mean.append(word)\n",
    "                elif word in wv.vocab:\n",
    "                    mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "                    all_words.add(wv.vocab[word].index)\n",
    "\n",
    "            if not mean:\n",
    "                return np.zeros(wv.vector_size, )\n",
    "\n",
    "            mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "            return mean\n",
    "        \n",
    "    def word_averaging_list(self, wv, text_list):\n",
    "                return np.vstack([self.word_averaging(wv, post) for post in text_list])\n",
    "        \n",
    "    def getRawData(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from preprocessing import PreProcessing\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text\n",
    "from keras import utils\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, tags):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.tags = tags\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, tfidf=True):\n",
    "        # TfidfVectorizer applies CountVectorizer to count word frequencies then\n",
    "        # applies TfidfTransformer to extract tfidf information for each token\n",
    "        if tfidf:\n",
    "            tfidf_vector = TfidfVectorizer()\n",
    "            self.X_train = tfidf_vector.fit_transform(self.X_train)\n",
    "            self.X_test = tfidf_vector.transform(self.X_test)\n",
    "\n",
    "        scoring = {'acc': 'accuracy',\n",
    "                   'prec_macro': 'precision_macro',\n",
    "                   'rec_micro': 'recall_macro',\n",
    "                   'f1_micro': 'f1_macro'}\n",
    "\n",
    "\n",
    "        scores = cross_validate(self.model,\n",
    "                    self.X_train,\n",
    "                    self.y_train,\n",
    "                    cv=5,\n",
    "                    scoring=scoring,\n",
    "                    return_train_score=True)\n",
    "\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        print('accuracy %s' % accuracy_score(self.y_test, y_pred))\n",
    "        print(classification_report(self.y_test, y_pred, target_names=self.tags))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(Model):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, tags):\n",
    "        Model.__init__(self, X_train, X_test, y_train, y_test, tags)\n",
    "        self.model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(Model):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, tags):\n",
    "        Model.__init__(self, X_train, X_test, y_train, y_test, tags)\n",
    "        self.model = LogisticRegression(n_jobs=1, C=1e5, solver='lbfgs', multi_class='auto', max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(Model):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, tags):\n",
    "        Model.__init__(self, X_train, X_test, y_train, y_test, tags)\n",
    "        self.model = SGDClassifier(max_iter=1000, tol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDeep:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, tags):\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.tags = tags\n",
    "        # Limit is used to get the most-frequent 500,000 word's vectors, so speed loading vectors a little.\n",
    "        glove_model = KeyedVectors.load_word2vec_format(\"pretrained_vectors/gensim_glove_vectors.txt\", binary=False,\n",
    "                                                        limit=500000)\n",
    "        # Used for initialization of model.syn0norm\n",
    "        glove_model.init_sims(replace=True)\n",
    "\n",
    "        print(\"Done Processing Pretrained Vectors\")\n",
    "\n",
    "        pre = PreProcessing()\n",
    "\n",
    "        test_tokenized = X_test.apply(lambda item: self.tokenize_text(item))\n",
    "        train_tokenized = X_train.apply(lambda item: self.tokenize_text(item))\n",
    "\n",
    "        self.X_train_word_average = pre.word_averaging_list(glove_model, train_tokenized)\n",
    "        self.X_test_word_average = pre.word_averaging_list(glove_model, test_tokenized)\n",
    "\n",
    "        print(\"Done Applying Pretrained Vectors\")\n",
    "\n",
    "    def train(self):\n",
    "        lr = LR(self.X_train_word_average, self.X_test_word_average, self.y_train, self.y_test, self.tags)\n",
    "        lr.train(tfidf=False)\n",
    "\n",
    "    # Tokenize Word\n",
    "    def tokenize_text(self, text):\n",
    "        tokens = []\n",
    "        for sent in nltk.sent_tokenize(text, language='english'):\n",
    "            for word in nltk.word_tokenize(sent, language='english'):\n",
    "                # To make sure that this is at least a word not single character\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                tokens.append(word)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWDeep:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, tags):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.max_words = 1000\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 3\n",
    "        self.num_classes = len(tags)\n",
    "\n",
    "        # Build the model (Neural Network)\n",
    "        # Sequential structure to store multiple sequential layers\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(512, input_shape=(self.max_words, )))\n",
    "        self.model.add(Activation('relu'))\n",
    "        # To prevent over fitting\n",
    "        # Dropout drops 50% of information so not to over fit model and improve performance\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes))\n",
    "        self.model.add(Activation('softmax'))\n",
    "        # Categorical cross entropy loss because we have multiple classes\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        tokenize = text.Tokenizer(num_words=self.max_words, char_level=False)\n",
    "        # Only fit on train, then use for testing\n",
    "        tokenize.fit_on_texts(X_train)\n",
    "\n",
    "        self.X_train = tokenize.texts_to_matrix(self.X_train)\n",
    "        self.X_test = tokenize.texts_to_matrix(self.X_test)\n",
    "\n",
    "        # label encoder to get one hot encoding for classes\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(self.y_train)\n",
    "        self.y_train = encoder.transform(self.y_train)\n",
    "        self.y_test = encoder.transform(self.y_test)\n",
    "\n",
    "        self.y_train = utils.to_categorical(self.y_train, self.num_classes)\n",
    "        self.y_test = utils.to_categorical(self.y_test, self.num_classes)\n",
    "\n",
    "        self.metrics = Metrics()\n",
    "\n",
    "    def train(self):\n",
    "        self.model.fit(self.X_train, self.y_train, epochs=self.epochs, batch_size=self.batch_size,\n",
    "                       verbose=True, validation_split=0.1,\n",
    "                       callbacks=[self.metrics,\n",
    "                                  EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "        score = self.model.evaluate(self.X_test, self.y_test,\n",
    "                                    batch_size=self.batch_size, verbose=True)\n",
    "        print('Test accuracy:', score[1])\n",
    "        return self.metrics.get_f1_scores()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, tags):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.epochs = 5\n",
    "        self.batch_size = 64\n",
    "        self.num_classes = len(tags)\n",
    "        # The maximum number of words to be used. Most frequeunt words\n",
    "        MAX_NB_WORDS = 50000\n",
    "        EMBEDDING_DIM = 100\n",
    "\n",
    "        self.metrics = Metrics()\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=self.X_train.shape[1]))\n",
    "        self.model.add(SpatialDropout1D(0.2))\n",
    "        self.model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "        self.model.add(Dense(20, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def train(self):\n",
    "        self.model.fit(self.X_train, self.y_train, epochs=self.epochs, batch_size=self.batch_size,\n",
    "                       verbose=True, validation_split=0.1,\n",
    "                       callbacks=[self.metrics,\n",
    "                                  EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "        score = self.model.evaluate(self.X_test, self.y_test,\n",
    "                                    batch_size=self.batch_size, verbose=True)\n",
    "        print('Test accuracy:', score[1])\n",
    "        return self.metrics.get_f1_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='micro')\n",
    "        _val_recall = recall_score(val_targ, val_predict, average='micro')\n",
    "        _val_precision = precision_score(val_targ, val_predict, average='micro')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\"— val_f1: {:f} — val_precision: {:f} — val_recall {:f}\".format(_val_f1, _val_precision, _val_recall))\n",
    "\n",
    "    def get_f1_scores(self):\n",
    "        return self.val_f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'dataset/stack-overflow-data.csv' does not exist: b'dataset/stack-overflow-data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b92b01899c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6412a77a9ae6>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPreProcessing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataset/stack-overflow-data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Extract unique classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thant\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thant\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thant\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thant\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thant\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'dataset/stack-overflow-data.csv' does not exist: b'dataset/stack-overflow-data.csv'"
     ]
    }
   ],
   "source": [
    "pre = PreProcessing()\n",
    "data = pre.clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['post']\n",
    "y = data['tags']\n",
    "# Split to 20% test data and 80% training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(X_train, X_test, y_train, y_test, pre.tags)\n",
    "scores = nb.train()\n",
    "results.append(scores['test_f1_micro'])\n",
    "names.append('Naive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR(X_train, X_test, y_train, y_test, pre.tags)\n",
    "scores = lr.train()\n",
    "\n",
    "results.append(scores['test_f1_micro'])\n",
    "names.append('LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM(X_train, X_test, y_train, y_test, pre.tags)\n",
    "scores = svm.train()\n",
    "\n",
    "results.append(scores['test_f1_micro'])\n",
    "names.append('SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2VecDeep(X_train, X_test, y_train, y_test, pre.tags)\n",
    "scores = wv.train()\n",
    "\n",
    "results.append(scores)\n",
    "names.append('W2V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BOWDeep(X_train, X_test, y_train, y_test, pre.tags)\n",
    "scores = bow.train()\n",
    "\n",
    "results.append(scores)\n",
    "names.append('BOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use different function for pre-processing\n",
    "X, y = pre.filter_rnn()\n",
    "# Split to 20% test data and 80% training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rnn = RNN(X_train, X_test, y_train, y_test, pre.tags)\n",
    "scores = rnn.train()\n",
    "\n",
    "results.append(scores)\n",
    "names.append('RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
